{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies and Import Libraries"
      ],
      "metadata": {
        "id": "4ld5BvAF_L_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers torch torchvision nltk rouge-score python-Levenshtein jiwer tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-TX7lgt_F2n",
        "outputId": "0d9c72c8-3327-466a-b2b2-faa787c1978c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PlTix89q5BLA"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from fuzzywuzzy import fuzz\n",
        "from jiwer import wer, cer\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "WTNeQgme_Icq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MAAaSxZ_JSv",
        "outputId": "190f1394-a59b-404c-cf45-3a17b83efa6a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQlDKNAnEdLK",
        "outputId": "fc0c0f9d-c22f-419d-bdd9-2c3bb7eb31e9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "7BReyQ2O_Yx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "file_path = '/content/drive/My Drive/JobSeeking/Orfium/normalization_assesment_dataset_10k.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Preview dataset\n",
        "print(\"Dataset preview:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m8T-ATD_WrL",
        "outputId": "b3fd3689-e348-4fcd-9b8b-e5dbd5b678fa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preview:\n",
            "                               raw_comp_writers_text  \\\n",
            "0            Jordan Riley/Adam Argyle/Martin Brammer   \n",
            "1                                      Martin Hygård   \n",
            "2  Jesse Robinson/Greg Phillips/Kishaun Bailey/Ka...   \n",
            "3                                     Mendel Brikman   \n",
            "4                                          Alvin Lee   \n",
            "\n",
            "                                          CLEAN_TEXT  \n",
            "0            Jordan Riley/Adam Argyle/Martin Brammer  \n",
            "1                                      Martin Hygård  \n",
            "2  Jesse Robinson/Greg Phillips/Kishaun Bailey/Ka...  \n",
            "3                                                NaN  \n",
            "4                                          Alvin Lee  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by:\n",
        "    - Lowercasing\n",
        "    - Removing special characters and numbers\n",
        "    - Stripping leading/trailing whitespaces\n",
        "    \"\"\"\n",
        "    if type(text) == float: # Handle NaN values\n",
        "      text = str(text)\n",
        "\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters and numbers\n",
        "    text = \" \".join(text.split())  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "raw_texts = df['raw_comp_writers_text'].apply(preprocess_text).tolist()\n",
        "clean_texts = df['CLEAN_TEXT'].apply(preprocess_text).tolist()"
      ],
      "metadata": {
        "id": "EdgqpWf7F4bt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer used in BERT (like the one from bert-base-uncased) is based on WordPiece tokenization, which is a subword-based method. It splits words into smaller units or subwords, especially for rare or out-of-vocabulary words. This helps BERT handle a large vocabulary without requiring explicit knowledge of every word in the language. The tokenizer starts by splitting words into the smallest units possible (characters or subword pieces), and then merges frequent pairs of tokens to form a vocabulary. This allows BERT to manage unknown or unseen words by breaking them into smaller, meaningful parts, ensuring efficient learning and generalization across diverse text data."
      ],
      "metadata": {
        "id": "rlE8YT65IgLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "l-DfcaRJ_bQp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TextNormalizationDataset class is a custom PyTorch dataset designed for text normalization tasks. It takes raw and clean text pairs, tokenizes them using a provided tokenizer, and returns the tokenized input IDs for both texts. This class supports padding and truncation to a specified maximum length for each text sequence."
      ],
      "metadata": {
        "id": "cr0_4G4xIpZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class TextNormalizationDataset(Dataset):\n",
        "    def __init__(self, raw_texts, clean_texts, tokenizer, max_len=50):\n",
        "        self.raw_texts = raw_texts\n",
        "        self.clean_texts = clean_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the raw and clean texts\n",
        "        raw_text = self.raw_texts[idx]\n",
        "        clean_text = self.clean_texts[idx]\n",
        "\n",
        "        # Tokenize the raw text and clean text\n",
        "        raw_encoded = self.tokenizer(\n",
        "            raw_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        clean_encoded = self.tokenizer(\n",
        "            clean_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Return the input ids (for raw and clean texts)\n",
        "        return raw_encoded[\"input_ids\"].squeeze(), clean_encoded[\"input_ids\"].squeeze()"
      ],
      "metadata": {
        "id": "9Jvw2JcrFSwH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split raw and clean texts into training and test sets\n",
        "train_raw, test_raw, train_clean, test_clean = train_test_split(\n",
        "    raw_texts, clean_texts, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Further split training data into training and validation sets\n",
        "train_raw, val_raw, train_clean, val_clean = train_test_split(\n",
        "    train_raw, train_clean, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets for train, validation, and test sets\n",
        "train_dataset = TextNormalizationDataset(train_raw, train_clean, tokenizer)\n",
        "val_dataset = TextNormalizationDataset(val_raw, val_clean, tokenizer)\n",
        "test_dataset = TextNormalizationDataset(test_raw, test_clean, tokenizer)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMKk6LXT_TiJ",
        "outputId": "ad4a0b0b-6906-4b98-df09-6ddfa3dc46a3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 7200\n",
            "Validation samples: 800\n",
            "Test samples: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Model"
      ],
      "metadata": {
        "id": "ATPZddbp_pMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Model\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        # Final fully connected layer to predict token probabilities\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Apply embedding and positional encoding\n",
        "        x = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        transformer_output = self.transformer_encoder(x)\n",
        "\n",
        "        # Pass through fully connected layer to get token predictions\n",
        "        predictions = self.fc(transformer_output)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "embed_dim = 128  # Dimension of embedding\n",
        "num_heads = 8    # Number of attention heads\n",
        "ff_dim = 512     # Feedforward dimension\n",
        "num_layers = 6   # Number of transformer encoder layers\n",
        "max_len = 50     # Maximum sequence length\n",
        "\n",
        "model = TransformerEncoderModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0B7AIQF_mMm",
        "outputId": "b3949315-fe82-46de-8fba-c20afe1e03d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "e5fPyTLY_ugf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "e4OU4VHEFIC4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with validation\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\")\n",
        "    for src, trg in progress_bar:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model\n",
        "        output = model(src)\n",
        "\n",
        "        # Reshape output and target for loss calculation\n",
        "        output = output.view(-1, vocab_size)\n",
        "        trg = trg.view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Update progress bar with current training loss\n",
        "        progress_bar.set_postfix({\"Train Loss\": train_loss / (progress_bar.n + 1)})\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
        "        for src, trg in progress_bar:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            # Forward pass through the model (for Transformer, we typically pass src)\n",
        "            output = model(src)\n",
        "\n",
        "            # Reshape output and target for loss calculation\n",
        "            output = output.view(-1, vocab_size)\n",
        "            trg = trg.view(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, trg)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Update progress bar with current validation loss\n",
        "            progress_bar.set_postfix({\"Val Loss\": val_loss / (progress_bar.n + 1)})\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Val Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTG0xvcQFGaU",
        "outputId": "392c1505-3bb3-4466-b746-ab077fd81a02"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5 - Training: 100%|██████████| 450/450 [00:21<00:00, 20.65it/s, Train Loss=5.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 5.5684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5 - Validation: 100%|██████████| 50/50 [00:01<00:00, 32.41it/s, Val Loss=3.66]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Val Loss: 3.5895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5 - Training: 100%|██████████| 450/450 [00:21<00:00, 21.22it/s, Train Loss=2.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 2.6930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5 - Validation: 100%|██████████| 50/50 [00:02<00:00, 22.41it/s, Val Loss=2.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Val Loss: 2.4959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5 - Training: 100%|██████████| 450/450 [00:20<00:00, 21.53it/s, Train Loss=1.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 1.7804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5 - Validation: 100%|██████████| 50/50 [00:00<00:00, 60.99it/s, Val Loss=2.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Val Loss: 2.2627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5 - Training: 100%|██████████| 450/450 [00:14<00:00, 31.82it/s, Train Loss=1.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 1.3831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5 - Validation: 100%|██████████| 50/50 [00:00<00:00, 60.57it/s, Val Loss=2.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Val Loss: 2.2714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5 - Training: 100%|██████████| 450/450 [00:14<00:00, 31.89it/s, Train Loss=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 1.1684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5 - Validation: 100%|██████████| 50/50 [00:00<00:00, 61.19it/s, Val Loss=2.35]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Val Loss: 2.3035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "DepNSk7P_1UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    bleu_scores, rouge_scores, jw_scores, exact_matches = [], [], [], []\n",
        "    wer_scores, cer_scores = [], []\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src)\n",
        "            output_ids = output.argmax(dim=-1)\n",
        "\n",
        "            for pred_ids, ref_ids in zip(output_ids, trg):\n",
        "                pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
        "                ref_text = tokenizer.decode(ref_ids, skip_special_tokens=True)\n",
        "\n",
        "                # BLEU\n",
        "                bleu_scores.append(sentence_bleu([ref_text.split()], pred_text.split()))\n",
        "\n",
        "                # ROUGE\n",
        "                rouge = scorer.score(ref_text, pred_text)\n",
        "                rouge_scores.append(rouge)\n",
        "\n",
        "                # Jaro-Winkler\n",
        "                jw_scores.append(fuzz.ratio(ref_text, pred_text))\n",
        "\n",
        "                # Exact Match\n",
        "                exact_matches.append(int(pred_text == ref_text))\n",
        "\n",
        "                # WER and CER\n",
        "                if ref_text and pred_text:\n",
        "                    wer_scores.append(wer(ref_text, pred_text))\n",
        "                    cer_scores.append(cer(ref_text, pred_text))\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
        "    avg_rouge1 = sum(r[\"rouge1\"].fmeasure for r in rouge_scores) / len(rouge_scores) if rouge_scores else 0.0\n",
        "    avg_rouge2 = sum(r[\"rouge2\"].fmeasure for r in rouge_scores) / len(rouge_scores) if rouge_scores else 0.0\n",
        "    avg_rougeL = sum(r[\"rougeL\"].fmeasure for r in rouge_scores) / len(rouge_scores) if rouge_scores else 0.0\n",
        "    avg_jw = sum(jw_scores) / len(jw_scores) if jw_scores else 0.0\n",
        "    avg_wer = sum(wer_scores) / len(wer_scores) if wer_scores else 0.0\n",
        "    avg_cer = sum(cer_scores) / len(cer_scores) if cer_scores else 0.0\n",
        "    avg_exact_match = sum(exact_matches) / len(exact_matches) if exact_matches else 0.0\n",
        "\n",
        "    print(f\"Test BLEU Score: {avg_bleu:.2f}\")\n",
        "    print(f\"Test ROUGE-1 Score: {avg_rouge1:.2f}\")\n",
        "    print(f\"Test ROUGE-2 Score: {avg_rouge2:.2f}\")\n",
        "    print(f\"Test ROUGE-L Score: {avg_rougeL:.2f}\")\n",
        "    print(f\"Test Jaro-Winkler Score: {avg_jw:.2f}\")\n",
        "    print(f\"Test WER: {avg_wer:.2f}\")\n",
        "    print(f\"Test CER: {avg_cer:.2f}\")\n",
        "    print(f\"Test Exact Match: {avg_exact_match:.2f}\")\n",
        "\n",
        "# Example usage:\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IZEeCrR_2yh",
        "outputId": "3b8558a3-4f44-4437-b90a-2399f0cd9735"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "Evaluating: 100%|██████████| 125/125 [00:03<00:00, 34.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test BLEU Score: 0.05\n",
            "Test ROUGE-1 Score: 0.59\n",
            "Test ROUGE-2 Score: 0.37\n",
            "Test ROUGE-L Score: 0.59\n",
            "Test Jaro-Winkler Score: 80.26\n",
            "Test WER: 0.76\n",
            "Test CER: 0.89\n",
            "Test Exact Match: 0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total runtime\n",
        "end_time = time.time()\n",
        "total_runtime_minutes = (end_time - start_time) / 60\n",
        "print(f\"Total Runtime: {total_runtime_minutes:.1f} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rceWW6ek_6so",
        "outputId": "efb7cb28-8513-42f4-b9b7-1267acd16550"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Runtime: 1.8 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5MwtD6lHjxC"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}